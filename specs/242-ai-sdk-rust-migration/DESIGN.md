# Design: AI SDK Rust Migration

**Parent Spec**: [242-ai-sdk-rust-migration](./README.md)

This document contains the detailed design specifications for migrating from Vercel AI SDK (Node.js) to aisdk.rs (Rust).

---

## Current Architecture (Pre-Migration)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rust HTTP Server        â”‚ â† leanspec-http (Axum)
â”‚ (leanspec-http)         â”‚   Manages state, routing
â”‚ â”œâ”€ Chat config          â”‚   
â”‚ â”œâ”€ Session persistence  â”‚   
â”‚ â””â”€ AI Worker Manager    â”‚   Spawns/manages worker
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ IPC (stdin/stdout, JSON Lines)
            â”‚ ~420 lines protocol handling
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node.js AI Worker       â”‚ â† @leanspec/ai-worker
â”‚ (packages/ai-worker/)   â”‚   Vercel AI SDK v6.0.39+
â”‚ â”œâ”€ streamText()         â”‚   14 LeanSpec tools
â”‚ â”œâ”€ Tool execution       â”‚   Requires Node.js v20+
â”‚ â””â”€ Provider factory     â”‚   
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Problems**:
- Users must have Node.js v20+ installed
- Two separate processes with IPC overhead
- Complex deployment (two runtimes)
- Error handling across process boundary
- Larger Docker images (~140MB Node.js base)

### Target Architecture (Post-Migration)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rust HTTP Server                â”‚ â† Single process
â”‚ (leanspec-http)                 â”‚   Pure Rust stack
â”‚ â”œâ”€ Chat config                  â”‚   
â”‚ â”œâ”€ Session persistence          â”‚   
â”‚ â””â”€ AI Module (native)           â”‚   Direct function calls
â”‚    â”œâ”€ aisdk.rs providers        â”‚   Zero IPC overhead
â”‚    â”œâ”€ 14 Tool implementations   â”‚   Type-safe
â”‚    â””â”€ Stream handling           â”‚   
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- âœ… No Node.js dependency
- âœ… Single binary deployment
- âœ… Zero IPC overhead
- âœ… Faster startup (no worker spawn)
- âœ… Better type safety (compile-time validation)
- âœ… Smaller binaries (~80MB vs ~140MB)
- âœ… Simpler architecture

### Integration with Spec 241 Architecture

Spec 241 consolidated infrastructure into `leanspec-core`. The AI module will follow this pattern:

```
rust/
â”œâ”€â”€ leanspec-core/          # Consolidated core library
â”‚   â”œâ”€â”€ src/ai/             # Existing (worker management)
â”‚   â”‚   â”œâ”€â”€ manager.rs      # Keep for process mgmt (deprecated)
â”‚   â”‚   â”œâ”€â”€ protocol.rs     # Delete (no IPC needed)
â”‚   â”‚   â””â”€â”€ worker.rs       # Delete (replaced by native)
â”‚   â””â”€â”€ src/ai_native/      # NEW - Native Rust AI
â”‚       â”œâ”€â”€ mod.rs          # Public API
â”‚       â”œâ”€â”€ chat.rs         # Chat streaming logic
â”‚       â”œâ”€â”€ providers.rs    # Provider factory
â”‚       â”œâ”€â”€ tools/          # 14 LeanSpec tools
â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚       â”‚   â”œâ”€â”€ list_specs.rs
â”‚       â”‚   â”œâ”€â”€ search_specs.rs
â”‚       â”‚   â”œâ”€â”€ get_spec.rs
â”‚       â”‚   â”œâ”€â”€ update_spec.rs
â”‚       â”‚   â”œâ”€â”€ link_specs.rs
â”‚       â”‚   â”œâ”€â”€ validate_specs.rs
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ error.rs        # AI-specific errors
â””â”€â”€ leanspec-http/          # HTTP layer
    â””â”€â”€ src/handlers/
        â””â”€â”€ chat_handler.rs # Updated to use ai_native
```

### aisdk.rs Integration

> **Note**: See [AI_SDK_ALIGNMENT.md](./AI_SDK_ALIGNMENT.md) for detailed SSE protocol specifications and frontend compatibility requirements.

The Rust implementation uses aisdk.rs for AI provider integration. The streaming protocol follows the Vercel AI SDK's `useChat` hook expectations with SSE events.

```rust
// rust/leanspec-core/src/ai_native/streaming.rs

use serde::{Deserialize, Serialize};

/// SSE stream events aligned with Vercel AI SDK frontend expectations
#[derive(Debug, Clone, Serialize)]
#[serde(tag = "type", rename_all = "kebab-case")]
pub enum StreamEvent {
    #[serde(rename = "start")]
    MessageStart {
        #[serde(rename = "messageId")]
        message_id: String,
    },
    
    #[serde(rename = "text-start")]
    TextStart { id: String },
    
    #[serde(rename = "text-delta")]
    TextDelta { id: String, delta: String },
    
    #[serde(rename = "text-end")]
    TextEnd { id: String },
    
    #[serde(rename = "tool-input-start")]
    ToolInputStart {
        #[serde(rename = "toolCallId")]
        tool_call_id: String,
        #[serde(rename = "toolName")]
        tool_name: String,
    },
    
    #[serde(rename = "tool-input-delta")]
    ToolInputDelta {
        #[serde(rename = "toolCallId")]
        tool_call_id: String,
        #[serde(rename = "inputTextDelta")]
        input_text_delta: String,
    },
    
    #[serde(rename = "tool-input-available")]
    ToolInputAvailable {
        #[serde(rename = "toolCallId")]
        tool_call_id: String,
        #[serde(rename = "toolName")]
        tool_name: String,
        input: serde_json::Value,
    },
    
    #[serde(rename = "tool-output-available")]
    ToolOutputAvailable {
        #[serde(rename = "toolCallId")]
        tool_call_id: String,
        output: serde_json::Value,
    },
    
    #[serde(rename = "start-step")]
    StartStep,
    
    #[serde(rename = "finish-step")]
    FinishStep,
    
    #[serde(rename = "finish")]
    Finish,
    
    #[serde(rename = "error")]
    Error {
        #[serde(rename = "errorText")]
        error_text: String,
    },
}

impl StreamEvent {
    pub fn to_sse_string(&self) -> String {
        let json = serde_json::to_string(self).expect("Failed to serialize event");
        format!("data: {}\n\n", json)
    }
}

pub fn sse_done() -> String {
    "data: [DONE]\n\n".to_string()
}
```

### Tool Implementation Strategy

**Approach**: Each tool is a separate Rust module with type-safe schema:

```rust
// rust/leanspec-core/src/ai_native/tools/list_specs.rs

use aisdk::macros::tool;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ListSpecsInput {
    pub project_id: Option<String>,
    pub status: Option<String>,
    pub priority: Option<String>,
    pub tags: Option<Vec<String>>,
}

#[tool(
    name = "list_specs",
    description = "List specs with optional filters"
)]
pub async fn list_specs(input: ListSpecsInput) -> Result<String, ToolError> {
    let project_id = input.project_id
        .ok_or_else(|| ToolError::MissingField("project_id"))?;
    
    // Call leanspec_core API directly
    let specs = crate::list_specs(
        &project_id,
        input.status.as_deref(),
        input.priority.as_deref(),
        input.tags.as_deref(),
    )?;
    
    Ok(serde_json::to_string(&specs)?)
}
```

### Provider Support

**Required Providers**:

1. **OpenRouter** - Priority for unified access
2. **OpenAI** - GPT-4o, GPT-4o-mini, GPT-5.2
3. **Anthropic** - Claude 3.5/4.5 Sonnet

```rust
// rust/leanspec-core/src/ai_native/providers.rs

use aisdk::providers::{OpenAI, Anthropic};

pub enum LeanSpecProvider {
    OpenRouter { api_key: String, base_url: String },
    OpenAI { api_key: String },
    Anthropic { api_key: String },
}

impl LeanSpecProvider {
    pub fn create_client(&self) -> Box<dyn LanguageModel> {
        match self {
            Self::OpenRouter { api_key, base_url } => {
                // OpenRouter uses OpenAI-compatible API
                Box::new(OpenAI::new(api_key).with_base_url(base_url))
            }
            Self::OpenAI { api_key } => {
                Box::new(OpenAI::new(api_key))
            }
            Self::Anthropic { api_key } => {
                Box::new(Anthropic::new(api_key))
            }
        }
    }
}
```

### HTTP Handler Integration

```rust
// rust/leanspec-http/src/handlers/chat_handler.rs

use axum::{
    body::Body,
    response::{Response, IntoResponse},
    http::{StatusCode, header},
    Json,
};
use futures::StreamExt;
use leanspec_core::ai_native::{stream_chat, ChatConfig, StreamEvent, sse_done};

pub async fn chat_handler(
    State(state): State<AppState>,
    Json(request): Json<ChatRequest>,
) -> Result<Response<Body>, ApiError> {
    // No IPC needed - direct function call!
    let stream = stream_chat(ChatConfig {
        messages: request.messages,
        provider_id: request.provider_id,
        model_id: request.model_id,
        system_prompt: request.system_prompt,
        max_steps: request.max_steps,
        tools_enabled: request.tools_enabled,
    }).await?;
    
    // Convert stream events to SSE format
    let sse_stream = stream
        .map(|event| event.to_sse_string())
        .chain(futures::stream::once(async { sse_done() }));
    
    // Create streaming response with required headers for AI SDK compatibility
    let response = Response::builder()
        .status(StatusCode::OK)
        .header(header::CONTENT_TYPE, "text/event-stream; charset=utf-8")
        .header(header::CACHE_CONTROL, "no-cache, no-transform")
        .header(header::CONNECTION, "keep-alive")
        .header("x-vercel-ai-ui-message-stream", "v1") // REQUIRED for frontend compatibility
        .body(Body::from_stream(sse_stream))?;
    
    Ok(response)
}

#[derive(Deserialize)]
struct ChatRequest {
    messages: Vec<UIMessage>,
    #[serde(rename = "providerId")]
    provider_id: String,
    #[serde(rename = "modelId")]
    model_id: String,
    #[serde(rename = "systemPrompt")]
    system_prompt: String,
    #[serde(rename = "maxSteps")]
    max_steps: u32,
    #[serde(rename = "toolsEnabled")]
    tools_enabled: bool,
}
```

### Migration Impact Analysis

**Files to Delete** (~1,200 LOC):
- âŒ `packages/ai-worker/` (entire package)
  - `src/worker.ts` (262 lines)
  - `src/tools/leanspec-tools.ts` (378 lines)
  - `src/provider-factory.ts` (50 lines)
  - `src/config.ts` (120 lines)
  - `package.json`, dependencies, etc.
- âŒ `rust/leanspec-core/src/ai/protocol.rs` (243 lines IPC protocol)
- âŒ `rust/leanspec-core/src/ai/worker.rs` (419 lines IPC worker)
- âŒ `rust/leanspec-http/src/handlers/chat_handler.rs` (IPC fallback logic)

**Files to Create** (~1,500 LOC):
- âœ… `rust/leanspec-core/src/ai_native/mod.rs` (~100 lines)
- âœ… `rust/leanspec-core/src/ai_native/chat.rs` (~300 lines)
- âœ… `rust/leanspec-core/src/ai_native/providers.rs` (~200 lines)
- âœ… `rust/leanspec-core/src/ai_native/tools/` (~800 lines, 14 tools Ã— ~60 lines)
- âœ… `rust/leanspec-core/src/ai_native/error.rs` (~100 lines)

**Files to Update**:
- ğŸ”„ `rust/leanspec-core/Cargo.toml` (add aisdk dependencies)
- ğŸ”„ `rust/leanspec-core/src/lib.rs` (export ai_native module)
- ğŸ”„ `rust/leanspec-http/src/handlers/chat_handler.rs` (use ai_native)
- ğŸ”„ `rust/leanspec-http/Cargo.toml` (remove IPC dependencies)
- ğŸ”„ Documentation, README files

**Net Change**: ~300 lines added (1,500 new - 1,200 deleted)

### Deployment Benefits

**Before** (Node.js + Rust):
```dockerfile
FROM node:20-slim
COPY --from=rust-builder /app/target/release/leanspec-http /usr/local/bin/
RUN npm install -g @leanspec/ai-worker
CMD ["leanspec-http"]
# Size: ~140MB
```

**After** (Pure Rust):
```dockerfile
FROM debian:bookworm-slim
COPY --from=rust-builder /app/target/release/leanspec-http /usr/local/bin/
CMD ["leanspec-http"]
# Size: ~80MB (-43%)
```

**Alpine (Static Build)**:
```dockerfile
FROM scratch
COPY --from=rust-builder /app/target/x86_64-unknown-linux-musl/release/leanspec-http /
CMD ["/leanspec-http"]
# Size: ~15MB (-89%)
```

