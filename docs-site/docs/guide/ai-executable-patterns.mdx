---
id: ai-executable-patterns
title: The 12 Patterns for AI-Executable Specs
sidebar_position: 2
---

> "Specs become leverage when both humans and autonomous agents can act on them without translation."

LeanSpec's innovation in the AI era is that a spec is not just a coordination artifact—it is an executable blueprint. These 12 patterns translate the core constraints and first principles into practical techniques that make specs reliably actionable by AI coding agents.

## Layer Context

Before applying patterns, remember the layered model:
- **Constraints** (discovered): Physics (context windows), Biology (working memory), Economics (token/time costs)
- **First Principles** (derived): Context Economy, Signal-to-Noise, Intent Over Implementation, Bridge the Gap, Progressive Disclosure
- **Patterns** (applied): The tools below that operationalize principles for execution

Each pattern lists: What it is, Why it matters for AI, How to apply, and a Quick Example.

---

## 1. Structure as Signal
 
**What**: Use a stable top-level section sequence: `## Problem`, `## Solution` (or Approach), `## Success Criteria`, optional `## Trade-offs`.

**Why (AI)**: Predictable anchors reduce parsing ambiguity and token waste.

**How**: Keep section headers exact; avoid creative synonyms that fragment training.

**Example**:
```markdown
## Problem
User onboarding time is high (avg 14m) due to multi-step form.
## Solution
Compress flow into 2 screens; defer profile enrichment.
## Success Criteria
Reduce avg onboarding to <5m; NPS +10; completion rate >92%.
```

## 2. Examples Over Abstraction
 
**What**: Prefer concrete payloads, CLI invocations, state transitions over descriptive prose.

**Why (AI)**: Few-shot learning: examples reduce hallucination risk.

**How**: For every described API/process, add at least one minimal working example.

**Example**:
```json
POST /api/invite {"email":"ceo@example.com","role":"admin"}
→ 201 {"id":"usr_123","status":"pending"}
```

## 3. Explicit Boundaries
 
**What**: State what is intentionally out of scope.

**Why (AI)**: Prevents speculative implementation and scope creep.

**How**: Add a `Out of Scope` bullet list under Solution or separate section.

**Example**:
```markdown
Out of Scope:
- Mobile app parity
- OAuth provider expansion
- Advanced analytics dashboard
```

## 4. Success Criteria First
 
**What**: Define measurable completion before listing implementation ideas.

**Why (AI)**: Guides agent planning; prevents optimizing the wrong dimension.

**How**: Use 3–5 crisp, testable criteria (latency target, adoption %, error rate).

**Example**:
```markdown
## Success Criteria
- P95 ingestion latency < 2s
- Zero data loss in synthetic stress test (1M events)
- Errors logged with correlated trace IDs
```

## 5. Intent Over Steps
 
**What**: Capture rationale and objectives, not micro task lists.

**Why (AI)**: Intent persists across iteration; steps shift with constraints.

**How**: Explain trade-offs; avoid over-specifying internal function names.

**Example**:
> We choose eventual consistency to favor ingestion throughput; strict ordering is not a requirement.

## 6. Parseable Metadata
 
**What**: Use frontmatter for machine-readable fields (status, priority, tags, relationships).

**Why (AI)**: Enables tooling to query, prioritize, and sequence work.

**How**: Keep system-managed fields untouched; add relationships (`depends_on`, `related`) explicitly.

**Example**:
```yaml
---
status: in-progress
priority: high
tags: [api, ingestion]
depends_on: [052-branding-assets]
---
```

## 7. Natural Language Body
 
**What**: Write clear prose for human comprehension; avoid cryptic shorthand.

**Why (AI)**: LLMs trained on natural language perform better with clear exposition.

**How**: Short paragraphs (≤4 sentences), active voice, eliminate filler.

**Example**:
> The current batch processor retries indefinitely. This hides systemic failures and inflates cost. We will cap retries to 5 and emit structured error events.

## 8. Concrete Over Theoretical
 
**What**: Prefer real data slices and actual edge cases encountered.

**Why (AI)**: Ground truth lowers hallucination and mismatched assumption risk.

**How**: Source examples from logs/tests; mark synthetic cases explicitly.

**Example**:
```text
Edge Case (Real): User deleted org mid invoice generation → orphan invoice.
Edge Case (Synthetic): Simulated 10K concurrent signups for rate limiter.
```

## 9. Constraints as Guardrails
 
**What**: Explicitly document hard limits (SLA, memory ceiling, cutoff dates).

**Why (AI)**: Prevents over-engineering and keeps solutions within viable bounds.

**How**: Add a `Constraints` subsection before design deep-dives.

**Example**:
```markdown
Constraints:
- Memory cap: 256MB per lambda invocation
- Cold start budget: <300ms
- Migration window closes 2025-12-15
```

## 10. Iteration Markers
 
**What**: Use clear placeholders (`TBD`, `OPEN QUESTION`, `PUNT`) instead of leaving ambiguity.

**Why (AI)**: Signals incomplete areas to avoid false certainty.

**How**: Uppercase markers; resolve or track before completion status change.

**Example**:
```markdown
OPEN QUESTION: Do we stream partial results or wait for full batch?
TBD: Rate limiting strategy for partner tier.
```

## 11. Learning Capture
 
**What**: Update spec reflecting what was actually built (delta reconciliation).

**Why (AI)**: Maintains single source; future agents rely on current truth.

**How**: After implementation, adjust Solution & Trade-offs; never leave divergences implicit.

**Example**:
> IMPLEMENTATION DELTA: Dropped Redis cache layer—Postgres query tuned to 14ms P95, cache unnecessary.

## 12. Single Source of Truth
 
**What**: Spec mirrors reality; no stale duplication in wikis/docs.

**Why (AI)**: Reduces conflicting inputs and tool misalignment.

**How**: Link outward, do not copy; archive superseded specs instead of editing silently.

**Example**:
> Archived: 033-cache-layer-design replaced by current ingestion spec.

---

## Applying the Patterns Together
1. Start with Problem & Success Criteria (Patterns 1 & 4)
2. Add constraints and boundaries (Patterns 3 & 9)
3. Provide examples early (Patterns 2 & 8)
4. Capture intent and rationale (Pattern 5)
5. Use iteration markers while velocity is high (Pattern 10)
6. Reconcile learning before marking complete (Pattern 11) → ensures Pattern 12

## Cross-Mapping to First Principles
| Pattern | Principle(s) Reinforced |
|---------|-------------------------|
| 1       | Context Economy, Signal-to-Noise |
| 2       | Bridge the Gap, Signal-to-Noise |
| 3       | Progressive Disclosure |
| 4       | Intent Over Implementation, Context Economy |
| 5       | Intent Over Implementation |
| 6       | Bridge the Gap, Progressive Disclosure |
| 7       | Bridge the Gap |
| 8       | Signal-to-Noise |
| 9       | Context Economy |
| 10      | Progressive Disclosure |
| 11      | Intent Over Implementation |
| 12      | Signal-to-Noise, Bridge the Gap |

## Quick Checklist
- [ ] Problem stated before solution
- [ ] Success criteria measurable
- [ ] At least one real example per API/process
- [ ] Boundaries & constraints explicit
- [ ] No stale TBDs at completion
- [ ] Spec updated post-implementation deltas

## Frequently Misapplied
| Anti-Pattern | Why It Fails | Fix |
|--------------|--------------|-----|
| Giant task list masquerading as solution | Loses intent & adaptability | Replace with rationale paragraphs |
| Absent success criteria | Hard to validate or prioritize | Define 3–5 quantifiable targets |
| Abstract pseudo-examples | Increases hallucination risk | Use real payloads/logs |
| Hidden constraints (only in code) | Promotes over-build | Surface them under Constraints |
| Stale divergence from implementation | Misleads future agents | Reconcile immediately after build |

## Next Steps
Move on to `When to Use` for judgment guidance, or revisit `Understanding LeanSpec` if patterns feel mechanical—grounding in constraints clarifies trade-offs.
